#!/usr/bin/env python3

"""
Supabase Google Data Refresh Script - PostgreSQL Version
====================================

This script refreshes Google data in Supabase by:
1. Deleting existing Google records from working_version_v3 table
2. Loading finalized data from E-created-db-data.json
3. Inserting fresh Google data into Supabase

Features:
- Direct PostgreSQL connection with pipeline_writer role
- Comprehensive error handling and logging
- Data validation and safety checks
- Bulk insert with transactions
- Rollback capability with backup

Author: AI Models Discovery Pipeline
Version: 2.0 (PostgreSQL + RLS)
Last Updated: 2025-10-02
"""

import os
import sys
import json
import logging
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

# Third-party imports
try:
    import psycopg2
    from psycopg2.extras import execute_batch, RealDictCursor
except ImportError:
    print("Error: psycopg2 package not found. Install with: pip install psycopg2-binary")
    sys.exit(1)

# Import database utilities
sys.path.append(str(Path(__file__).parent.parent.parent))
try:
    from db_utils import (
        get_pipeline_db_connection,
        get_record_count,
        backup_records,
        delete_records,
        insert_records_batch
    )
except ImportError:
    print("Error: db_utils.py not found in project root")
    sys.exit(1)

# Load environment variables
try:
    from dotenv import load_dotenv
    env_paths = [
        Path(__file__).parent.parent.parent / ".env.local",
        Path(__file__).parent.parent / ".env"
    ]
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path)
            print(f"‚úÖ Loaded environment variables from {env_path}")
            break
except ImportError:
    print("‚ö†Ô∏è python-dotenv not installed")

# Configuration
SCRIPT_DIR = Path(__file__).parent
JSON_FILE = SCRIPT_DIR / "../02_outputs" / "E-created-db-data.json"
LOG_FILE = SCRIPT_DIR / "../02_outputs/refresh-supabase-working-version-report.txt"

# Database configuration
TABLE_NAME = "working_version_v3"
INFERENCE_PROVIDER = "Google"

# Setup logging
def setup_logging():
    with open(LOG_FILE, 'w', encoding='utf-8') as f:
        f.write(f"Supabase {INFERENCE_PROVIDER} Working Version Refresh Report\n")
        f.write(f"Last Run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 60 + "\n\n")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(LOG_FILE, mode='a'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()


def load_finalized_json() -> Optional[List[Dict[str, Any]]]:
    logger.info(f"üìÅ Loading finalized JSON data from {JSON_FILE}...")

    if not JSON_FILE.exists():
        logger.error(f"‚ùå JSON file not found: {JSON_FILE}")
        return None

    try:
        with open(JSON_FILE, 'r', encoding='utf-8') as file:
            data = json.load(file)

        models = data if isinstance(data, list) else data.get('models', [])
        if not models:
            logger.error(f"‚ùå No models found in JSON")
            return None

        valid_models = [m for m in models if m.get('inference_provider') == INFERENCE_PROVIDER]
        if not valid_models:
            logger.error(f"‚ùå No valid {INFERENCE_PROVIDER} models found in JSON")
            return None

        logger.info(f"‚úÖ Loaded {len(valid_models)} valid {INFERENCE_PROVIDER} models from JSON")
        return valid_models

    except Exception as e:
        logger.error(f"‚ùå Failed to load JSON data: {str(e)}")
        return None


def prepare_data_for_insert(models: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    logger.info("üßπ Preparing data for database insertion...")

    prepared_models = []
    auto_managed_fields = ['id', 'created_at', 'updated_at']

    for model in models:
        clean_model = {k: v for k, v in model.items() if k not in auto_managed_fields}
        nullable_fields = ['license_info_text', 'license_info_url']
        for field in nullable_fields:
            if field in clean_model and clean_model[field] is not None and not str(clean_model[field]).strip():
                clean_model[field] = None
        prepared_models.append(clean_model)

    logger.info(f"‚úÖ Prepared {len(prepared_models)} models for insertion")
    return prepared_models


def restore_backup_data(conn, backup_data: List[Dict[str, Any]]) -> bool:
    logger.info(f"üîÑ Rolling back: Restoring {len(backup_data)} {INFERENCE_PROVIDER} records...")

    if not backup_data:
        logger.info("‚úÖ No data to restore")
        return True

    auto_managed_fields = ['id', 'created_at', 'updated_at']
    clean_backup = [{k: v for k, v in record.items() if k not in auto_managed_fields}
                    for record in backup_data]

    if insert_records_batch(conn, TABLE_NAME, clean_backup, batch_size=50):
        final_count = get_record_count(conn, TABLE_NAME, INFERENCE_PROVIDER)
        logger.info(f"‚úÖ Rollback successful: {final_count} {INFERENCE_PROVIDER} records restored")
        return True
    else:
        logger.error(f"‚ùå Rollback failed")
        return False


def main():
    logger.info("=" * 60)
    logger.info(f"SUPABASE {INFERENCE_PROVIDER.upper()} DATA REFRESH STARTED")
    logger.info("=" * 60)
    start_time = datetime.now()
    conn = None
    backup_data = None

    try:
        conn = get_pipeline_db_connection()
        if not conn:
            logger.error("‚ùå REFRESH FAILED: Could not connect to database")
            return False

        logger.info("‚úÖ Database connection established")
        logger.info(f"   Target table: {TABLE_NAME}")

        initial_count = get_record_count(conn, TABLE_NAME, INFERENCE_PROVIDER)
        if initial_count is None:
            logger.error("‚ùå REFRESH FAILED: Could not query initial state")
            return False

        models = load_finalized_json()
        if not models:
            logger.error("‚ùå REFRESH FAILED: Could not load JSON data")
            return False

        prepared_models = prepare_data_for_insert(models)
        if not prepared_models:
            logger.error("‚ùå REFRESH FAILED: No valid models to insert")
            return False

        logger.info("üõ°Ô∏è CREATING BACKUP FOR ROLLBACK PROTECTION...")
        backup_data = backup_records(conn, TABLE_NAME, INFERENCE_PROVIDER)
        if backup_data is None:
            logger.error("‚ùå REFRESH FAILED: Could not backup existing data - ABORTING")
            return False

        logger.info(f"‚úÖ Backed up {len(backup_data)} existing {INFERENCE_PROVIDER} records")

        logger.info(f"üóëÔ∏è Deleting existing {INFERENCE_PROVIDER} records from {TABLE_NAME}...")
        if not delete_records(conn, TABLE_NAME, INFERENCE_PROVIDER):
            logger.error("‚ùå REFRESH FAILED: Could not delete existing data")
            return False

        logger.info(f"‚úÖ Successfully deleted {initial_count} {INFERENCE_PROVIDER} records")

        logger.info(f"üì§ Inserting {len(prepared_models)} models into {TABLE_NAME}...")
        if not insert_records_batch(conn, TABLE_NAME, prepared_models, batch_size=100):
            logger.error("‚ùå REFRESH FAILED: Data insertion failed - INITIATING ROLLBACK")
            if restore_backup_data(conn, backup_data):
                logger.info("‚úÖ ROLLBACK SUCCESSFUL: Original data restored")
            else:
                logger.error("‚ùå ROLLBACK FAILED: Manual intervention required!")
            return False

        logger.info(f"‚úÖ Successfully inserted {len(prepared_models)} models")

        logger.info("üîç Verifying insertion results...")
        final_count = get_record_count(conn, TABLE_NAME, INFERENCE_PROVIDER)
        if final_count != len(prepared_models):
            logger.error(f"‚ùå Verification failed: Expected {len(prepared_models)}, found {final_count}")
            logger.error("‚ùå REFRESH FAILED: Verification failed - INITIATING ROLLBACK")
            if restore_backup_data(conn, backup_data):
                logger.info("‚úÖ ROLLBACK SUCCESSFUL: Original data restored")
            else:
                logger.error("‚ùå ROLLBACK FAILED: Manual intervention required!")
            return False

        end_time = datetime.now()
        duration = end_time - start_time

        logger.info("=" * 60)
        logger.info(f"üéâ SUPABASE {INFERENCE_PROVIDER.upper()} DATA REFRESH COMPLETED SUCCESSFULLY")
        logger.info("=" * 60)
        logger.info(f"üìä Summary:")
        logger.info(f"   ‚Ä¢ Initial {INFERENCE_PROVIDER} records: {initial_count}")
        logger.info(f"   ‚Ä¢ Records backed up: {len(backup_data)}")
        logger.info(f"   ‚Ä¢ Records deleted: {initial_count}")
        logger.info(f"   ‚Ä¢ New records inserted: {len(prepared_models)}")
        logger.info(f"   ‚Ä¢ Final record count: {final_count}")
        logger.info(f"   ‚Ä¢ Processing time: {duration}")
        logger.info(f"   ‚Ä¢ Report file: {LOG_FILE}")
        logger.info("=" * 60)

        return True

    except Exception as e:
        logger.error(f"‚ùå UNEXPECTED ERROR: {str(e)}")
        if backup_data and conn:
            logger.info("üîÑ Attempting emergency rollback...")
            if restore_backup_data(conn, backup_data):
                logger.info("‚úÖ EMERGENCY ROLLBACK SUCCESSFUL")
            else:
                logger.error("‚ùå EMERGENCY ROLLBACK FAILED")
        return False

    finally:
        if conn:
            conn.close()
            logger.info("Database connection closed")


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Operation interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {str(e)}")
        sys.exit(1)
